{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "baseline2 (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NOTEBOOK : The Eternals Donkey of ML\n",
        "\n",
        "To run this notebook read README.md"
      ],
      "metadata": {
        "id": "G_-TSQGjvexi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preparation"
      ],
      "metadata": {
        "id": "TQRm44tNrrTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Import packages"
      ],
      "metadata": {
        "id": "-ti9w2p6rrTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.linear_model import Lasso\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "import ast\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "!pip install stellargraph[gpu]\n",
        "\n",
        "from stellargraph.data import BiasedRandomWalk\n",
        "from stellargraph import StellarGraph\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        " \n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-12-08T15:08:29.975307Z",
          "iopub.execute_input": "2021-12-08T15:08:29.975936Z",
          "iopub.status.idle": "2021-12-08T15:08:56.740078Z",
          "shell.execute_reply.started": "2021-12-08T15:08:29.975838Z",
          "shell.execute_reply": "2021-12-08T15:08:56.739269Z"
        },
        "trusted": true,
        "id": "y6qP-nafrrT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Define constants"
      ],
      "metadata": {
        "id": "qO7tuF-vrrT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "mediane = 6\n",
        "\n",
        "# ABSTRACT EMBEDDINGS\n",
        "NB_FEATURES_ABSTRACTS = 512\n",
        "METHOD = np.mean # to combine several abstract embeddings\n",
        "\n",
        "# RANDOM WALKS\n",
        "LEN_WALK = 25\n",
        "NB_WALK_PER_NODE = 20\n",
        "USESTD = True\n",
        "USEMEAN = True\n",
        "QUANTILES = [0.1,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.9]\n",
        "\n",
        "# STORAGE\n",
        "FILE_FEATURES_GRAPH    = \"features_graph\"\n",
        "FILE_FEATURES_ABSTRACT = \"features_abstracts\"\n",
        "FILE_DOCUMENT          = \"documents\"\n",
        "FILE_ABSTRACTS         = \"abstracts_text\"\n",
        "FILE_EMBEDDINGS        = \"embeddings\"\n",
        "\n",
        "# Network\n",
        "USE_NORMALIZER = True\n",
        "USE_LINEAR_NORMALIZER_Y = True\n",
        "NB_EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.0002"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T15:08:56.741707Z",
          "iopub.execute_input": "2021-12-08T15:08:56.743468Z",
          "iopub.status.idle": "2021-12-08T15:08:56.750801Z",
          "shell.execute_reply.started": "2021-12-08T15:08:56.743426Z",
          "shell.execute_reply": "2021-12-08T15:08:56.750065Z"
        },
        "trusted": true,
        "id": "EZsh4zYdrrT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NB_FEATURES_GRAPH = 3 + 2*USESTD + 2*USEMEAN + 2*len(QUANTILES)\n",
        "NB_FEATURES = NB_FEATURES_GRAPH + NB_FEATURES_ABSTRACTS"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T15:08:56.751892Z",
          "iopub.execute_input": "2021-12-08T15:08:56.752459Z",
          "iopub.status.idle": "2021-12-08T15:08:56.760001Z",
          "shell.execute_reply.started": "2021-12-08T15:08:56.752417Z",
          "shell.execute_reply": "2021-12-08T15:08:56.759272Z"
        },
        "trusted": true,
        "id": "QQi73sXurrT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Read data"
      ],
      "metadata": {
        "id": "oStEt90nrrT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read training data\n",
        "df_train = pd.read_csv('/kaggle/input/inf554-2021/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
        "n = df_train.shape[0]\n",
        "print(\"n =\", n)\n",
        "\n",
        "# read test data\n",
        "df_test = pd.read_csv('/kaggle/input/inf554-2021/test.csv', dtype={'author': np.int64})\n",
        "n_test = df_test.shape[0]\n",
        "print(\"n_test =\", n_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T15:08:56.763175Z",
          "iopub.execute_input": "2021-12-08T15:08:56.763465Z",
          "iopub.status.idle": "2021-12-08T15:08:56.884484Z",
          "shell.execute_reply.started": "2021-12-08T15:08:56.763438Z",
          "shell.execute_reply": "2021-12-08T15:08:56.883120Z"
        },
        "trusted": true,
        "id": "E6k3R8p5rrT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Features selection"
      ],
      "metadata": {
        "id": "GrqdEzKnrrT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Vectorization of abstacts"
      ],
      "metadata": {
        "id": "QgX6JunirrT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1. Preprocessing of abstracts"
      ],
      "metadata": {
        "id": "atVNWXL5rrT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function converts the abstracts from the inverted index format to readable text"
      ],
      "metadata": {
        "id": "VE4TghTjrrT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rearrange_text(inverted):\n",
        "    liste = [\"\" for k in range(inverted['IndexLength'])]\n",
        "    \n",
        "    for k in inverted['InvertedIndex']:\n",
        "        for i in inverted['InvertedIndex'][k]:\n",
        "            liste[i] = k\n",
        "    text = \"\"\n",
        "    for k in liste:\n",
        "        text += k\n",
        "        text += \" \"\n",
        "    return text[:-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:40.964946Z",
          "iopub.execute_input": "2021-12-08T13:02:40.965764Z",
          "iopub.status.idle": "2021-12-08T13:02:40.971495Z",
          "shell.execute_reply.started": "2021-12-08T13:02:40.96572Z",
          "shell.execute_reply": "2021-12-08T13:02:40.970677Z"
        },
        "trusted": true,
        "id": "h-hLvhLmrrT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_abstracts():\n",
        "    abstract = {}\n",
        "    with open('/kaggle/input/abstracts/abstracts.txt') as f:\n",
        "        for k in tqdm(f.readlines()):\n",
        "            part = k.split('----')\n",
        "            try :\n",
        "                abstract[int(part[0])] = ast.literal_eval(part[1])\n",
        "                abstract[int(part[0])] = rearrange_text(abstract[int(part[0])])\n",
        "            except:\n",
        "                continue"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:41.909505Z",
          "iopub.execute_input": "2021-12-08T13:02:41.910089Z",
          "iopub.status.idle": "2021-12-08T13:02:41.920598Z",
          "shell.execute_reply.started": "2021-12-08T13:02:41.910045Z",
          "shell.execute_reply": "2021-12-08T13:02:41.919905Z"
        },
        "trusted": true,
        "id": "31158RVSrrT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_abstracts(abstracts):\n",
        "    with open(FILE_ABSTRACTS + '.pkl', 'wb') as f:\n",
        "        pickle.dump(abstracts, f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:42.208294Z",
          "iopub.execute_input": "2021-12-08T13:02:42.208531Z",
          "iopub.status.idle": "2021-12-08T13:02:42.21232Z",
          "shell.execute_reply.started": "2021-12-08T13:02:42.208492Z",
          "shell.execute_reply": "2021-12-08T13:02:42.211635Z"
        },
        "trusted": true,
        "id": "o5swahlyrrT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_abstracts(name):\n",
        "    documabstractsents = None\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        abstracts = pickle.load(f)\n",
        "    return abstracts"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:42.628096Z",
          "iopub.execute_input": "2021-12-08T13:02:42.628326Z",
          "iopub.status.idle": "2021-12-08T13:02:42.63276Z",
          "shell.execute_reply.started": "2021-12-08T13:02:42.628299Z",
          "shell.execute_reply": "2021-12-08T13:02:42.631829Z"
        },
        "trusted": true,
        "id": "KaWW1IfVrrT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to not compute every time :"
      ],
      "metadata": {
        "id": "MZOmtr5bsECV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"/kaggle/input/saved-data/\" + FILE_ABSTRACTS + '.pkl'):\n",
        "    abstract = load_abstracts(\"/kaggle/input/saved-data/\" + FILE_ABSTRACTS)\n",
        "    print(\"Abstracts loaded from saved dataset\")\n",
        "elif os.path.isfile(FILE_ABSTRACTS + '.pkl'):\n",
        "    abstract = load_abstracts(FILE_ABSTRACTS)\n",
        "    print(\"Abstracts loaded from temporary save\")\n",
        "else:\n",
        "    abstract = compute_abstracts()\n",
        "    save_abstracts(abstract)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:43.139243Z",
          "iopub.execute_input": "2021-12-08T13:02:43.139483Z",
          "iopub.status.idle": "2021-12-08T13:02:48.594019Z",
          "shell.execute_reply.started": "2021-12-08T13:02:43.139456Z",
          "shell.execute_reply": "2021-12-08T13:02:48.593222Z"
        },
        "trusted": true,
        "id": "xipr6-TjrrUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a Hashmap associating an author id to the list of papers id that he wrote"
      ],
      "metadata": {
        "id": "z5eiW9qYsjyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "authors = {}\n",
        "with open('/kaggle/input/inf554-2021/author_papers.txt') as f:\n",
        "    for k in tqdm(f.readlines()):\n",
        "        part = k.split(':')\n",
        "        authors[int(part[0])] = [int(i) for i in part[1].split('-')]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:48.602461Z",
          "iopub.execute_input": "2021-12-08T13:02:48.602966Z",
          "iopub.status.idle": "2021-12-08T13:02:49.833444Z",
          "shell.execute_reply.started": "2021-12-08T13:02:48.602929Z",
          "shell.execute_reply": "2021-12-08T13:02:49.832672Z"
        },
        "trusted": true,
        "id": "e0EP8h5trrUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a hashmap of associating a paper id to its position in ```X_paper```"
      ],
      "metadata": {
        "id": "FQP08UuEssr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/inf554-2021/train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
        "\n",
        "X_paper = []\n",
        "papers = {}\n",
        "\n",
        "j = 0\n",
        "for i,row in tqdm(df.iterrows()):\n",
        "    author = int(row[\"author\"])\n",
        "    if author in authors:\n",
        "        for paper in authors[author]:\n",
        "            if paper in abstract:\n",
        "                X_paper.append(abstract[paper])\n",
        "                papers[paper] = j\n",
        "                j += 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:49.835351Z",
          "iopub.execute_input": "2021-12-08T13:02:49.836157Z",
          "iopub.status.idle": "2021-12-08T13:02:58.39329Z",
          "shell.execute_reply.started": "2021-12-08T13:02:49.836116Z",
          "shell.execute_reply": "2021-12-08T13:02:58.392526Z"
        },
        "trusted": true,
        "id": "xb2NLdNDrrUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleans the abstracts from special characters :"
      ],
      "metadata": {
        "id": "HsPRnJw8tIXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "\n",
        "def compute_documents():\n",
        "    documents = []\n",
        "    stemmer = WordNetLemmatizer()\n",
        "\n",
        "    for sen in tqdm(range(0, len(X_paper))):\n",
        "        # Remove all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(X_paper[sen]))\n",
        "\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # Lemmatization\n",
        "        document = document.split()\n",
        "\n",
        "        document = [stemmer.lemmatize(word) for word in document]\n",
        "        document = ' '.join(document)\n",
        "\n",
        "        documents.append(document)\n",
        "        \n",
        "    return documents"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:18:10.218105Z",
          "iopub.execute_input": "2021-12-07T14:18:10.21839Z",
          "iopub.status.idle": "2021-12-07T14:18:10.270761Z",
          "shell.execute_reply.started": "2021-12-07T14:18:10.218356Z",
          "shell.execute_reply": "2021-12-07T14:18:10.269485Z"
        },
        "trusted": true,
        "id": "U7C2chYBrrUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_documents(documents):\n",
        "    with open(FILE_DOCUMENT + '.pkl', 'wb') as f:\n",
        "        pickle.dump(documents, f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:18:10.272238Z",
          "iopub.execute_input": "2021-12-07T14:18:10.27543Z",
          "iopub.status.idle": "2021-12-07T14:18:10.281934Z",
          "shell.execute_reply.started": "2021-12-07T14:18:10.275381Z",
          "shell.execute_reply": "2021-12-07T14:18:10.280685Z"
        },
        "trusted": true,
        "id": "o2ptG9XCrrUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(name):\n",
        "    documents = None\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        documents = pickle.load(f)\n",
        "    return documents"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:18:10.28797Z",
          "iopub.execute_input": "2021-12-07T14:18:10.288508Z",
          "iopub.status.idle": "2021-12-07T14:18:10.294435Z",
          "shell.execute_reply.started": "2021-12-07T14:18:10.288472Z",
          "shell.execute_reply": "2021-12-07T14:18:10.293434Z"
        },
        "trusted": true,
        "id": "qWS_szR0rrUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"/kaggle/input/saved-data/\" + FILE_DOCUMENT + '.pkl'):\n",
        "    documents = load_documents(\"/kaggle/input/saved-data/\" + FILE_DOCUMENT)\n",
        "    print(\"Documents loaded from saved dataset\")\n",
        "elif os.path.isfile(FILE_ABSTRACTS + '.pkl'):\n",
        "    documents = load_documents(FILE_DOCUMENT)\n",
        "    print(\"Documents loaded from temporary save\")\n",
        "else:\n",
        "    documents = compute_documents()\n",
        "    save_documents(documents)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:18:10.296161Z",
          "iopub.execute_input": "2021-12-07T14:18:10.296687Z",
          "iopub.status.idle": "2021-12-07T14:18:19.374169Z",
          "shell.execute_reply.started": "2021-12-07T14:18:10.296651Z",
          "shell.execute_reply": "2021-12-07T14:18:19.373435Z"
        },
        "trusted": true,
        "id": "nafNEEWnrrUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.2. Word2Vec\n",
        "\n",
        "Here we will use Word2Vec to transform a word into a vector of 300 components.\n",
        "\n",
        "To get the embedding of an abstract we simply compute the mean of the embeddigns of word in the abstract.\n",
        "\n",
        "then for an autour we have to merge the embeddings of each abstracts. The method of merging can be chosen thanks to ```METHOD```. The best one seems to be the mean"
      ],
      "metadata": {
        "id": "gxhyyPZ0rrT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_sentence(sentence, model):\n",
        "    vectors = []\n",
        "    for w in sentence:\n",
        "        if w in vocab:\n",
        "            vectors.append(model[w])\n",
        "    return np.mean(vectors, axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T15:09:00.094008Z",
          "iopub.execute_input": "2021-12-08T15:09:00.094579Z",
          "iopub.status.idle": "2021-12-08T15:09:00.100443Z",
          "shell.execute_reply.started": "2021-12-08T15:09:00.094538Z",
          "shell.execute_reply": "2021-12-08T15:09:00.099719Z"
        },
        "trusted": true,
        "id": "BQm9Rm2mrrT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the performance of word2vec, we remove stopwords (common words such as *is*, *the*, ...) and lemmatize the words."
      ],
      "metadata": {
        "id": "P6L7-OerrrT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: stackoverflow\n",
        "# method to clean the reviews, tokenize, remove stop words and lemmatize them.\n",
        "def embed(list_sentence, model):\n",
        "    X_embeded = np.zeros((len(list_sentence), 300))\n",
        "\n",
        "    for (index,sent) in enumerate(list_sentence):\n",
        "        \n",
        "        #remove html content\n",
        "        review_text = BeautifulSoup(sent).get_text()\n",
        "        \n",
        "        #remove non-alphabetic characters\n",
        "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    \n",
        "        #tokenize the sentences\n",
        "        words = word_tokenize(review_text.lower())\n",
        "    \n",
        "        #stop words removal\n",
        "        omit_words = set(stopwords.words('english'))\n",
        "        words = [x for x in words if x not in omit_words]\n",
        "        \n",
        "        #lemmatize each word to its lemma\n",
        "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
        "        X_embeded[index] = embed_sentence(lemma_words, model)\n",
        "\n",
        "    return X_embeded"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T15:09:02.336107Z",
          "iopub.execute_input": "2021-12-08T15:09:02.336699Z",
          "iopub.status.idle": "2021-12-08T15:09:02.343854Z",
          "shell.execute_reply.started": "2021-12-08T15:09:02.336659Z",
          "shell.execute_reply": "2021-12-08T15:09:02.342743Z"
        },
        "trusted": true,
        "id": "1XqrdM1irrT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features_abstract(df, start=0):\n",
        "    total_rows = len(df.index)\n",
        "    X_emb = np.zeros((total_rows, NB_FEATURES_ABSTRACTS))\n",
        "\n",
        "    for j,row in tqdm(df.iterrows()):\n",
        "            i = j - start\n",
        "            author = row['author']\n",
        "            list_papers  = authors[author]\n",
        "            list_indexes = [papers[p] for p in list_papers if p in papers]\n",
        "            list_papers = [X_paper[i] for i in list_indexes]\n",
        "            list_embeddings = embed(list_papers, model)\n",
        "            embeding = METHOD(np.array(list_embeddings), axis=0)\n",
        "            X_emb[i] = embeding\n",
        "\n",
        "    return X_emb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T15:09:05.200208Z",
          "iopub.execute_input": "2021-12-08T15:09:05.201071Z",
          "iopub.status.idle": "2021-12-08T15:09:05.208824Z",
          "shell.execute_reply.started": "2021-12-08T15:09:05.200997Z",
          "shell.execute_reply": "2021-12-08T15:09:05.207688Z"
        },
        "trusted": true,
        "id": "uRiDwqaPrrUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this if you already have the folder of embeddings computed :"
      ],
      "metadata": {
        "id": "QWqfAGHGrrUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_emb_new = np.load(\"../input/saved-data/embeddings_new.npy\")\n",
        "X_test_emb_new = np.load(\"./embeddings_test_new.npy\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T15:09:10.646774Z",
          "iopub.execute_input": "2021-12-08T15:09:10.647127Z",
          "iopub.status.idle": "2021-12-08T15:09:15.853882Z",
          "shell.execute_reply.started": "2021-12-08T15:09:10.647082Z",
          "shell.execute_reply": "2021-12-08T15:09:15.852914Z"
        },
        "trusted": true,
        "id": "MDaLhyYsrrUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have not the files, you need to load the model and then compute the embeddings (loading the model can take up to 30 minutes)"
      ],
      "metadata": {
        "id": "v20Rzoz7rrUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Word2Vec, we used a pretrained model as it took more than 100hours to train on our abstracts.\n",
        "\n",
        "The model used was trained on 5.8billions token (LexVec).\n",
        "\n",
        "It can be found here : https://github.com/alexandres/lexvec\n"
      ],
      "metadata": {
        "id": "tSVIS-8qrrUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('../input/word2vec/vectors.txt', binary=False)\n",
        "vocab = model.key_to_index "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:02:59.83406Z",
          "iopub.execute_input": "2021-12-08T13:02:59.834563Z",
          "iopub.status.idle": "2021-12-08T13:18:54.016077Z",
          "shell.execute_reply.started": "2021-12-08T13:02:59.834526Z",
          "shell.execute_reply": "2021-12-08T13:18:54.015284Z"
        },
        "trusted": true,
        "id": "Cr9UPFejrrUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just a test to try out Word2Vec :"
      ],
      "metadata": {
        "id": "qthGEAWBrrUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "a = model[\"king\"] - model[\"man\"] + model[\"woman\"]\n",
        "b = model[\"queen\"]\n",
        "sim = 1 - spatial.distance.cosine(a, b)\n",
        "print(sim)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:23:37.358584Z",
          "iopub.execute_input": "2021-12-08T13:23:37.358878Z",
          "iopub.status.idle": "2021-12-08T13:23:37.364868Z",
          "shell.execute_reply.started": "2021-12-08T13:23:37.35884Z",
          "shell.execute_reply": "2021-12-08T13:23:37.363965Z"
        },
        "trusted": true,
        "id": "2OCN9xoWrrUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute the embeddings :"
      ],
      "metadata": {
        "id": "ezTtj-jhrrUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NB_FEATURES_ABSTRACTS = 300\n",
        "X_test_emb_new = create_features_abstract(df_test)\n",
        "X_test_emb = create_features_abstract(df_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:33:28.984381Z",
          "iopub.execute_input": "2021-12-08T13:33:28.985243Z",
          "iopub.status.idle": "2021-12-08T13:33:37.16976Z",
          "shell.execute_reply.started": "2021-12-08T13:33:28.985186Z",
          "shell.execute_reply": "2021-12-08T13:33:37.168518Z"
        },
        "trusted": true,
        "id": "-VQ07IC8rrUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to save them :"
      ],
      "metadata": {
        "id": "LKM2w532rrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"embeddings_new.npy\", X_emb_new)\n",
        "np.save(\"embeddings_test_new.npy\", X_test_emb_new)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:30:23.993082Z",
          "iopub.execute_input": "2021-12-08T13:30:23.994993Z",
          "iopub.status.idle": "2021-12-08T13:30:24.120971Z",
          "shell.execute_reply.started": "2021-12-08T13:30:23.994953Z",
          "shell.execute_reply": "2021-12-08T13:30:24.120138Z"
        },
        "trusted": true,
        "id": "7DhLNUm0rrUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3. TFIDF"
      ],
      "metadata": {
        "id": "eShyxW55sKtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(documents):\n",
        "    tfidfconverter = TfidfVectorizer(max_features=NB_FEATURES_ABSTRACTS, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "    X_paper_vect = tfidfconverter.fit_transform(documents).toarray()\n",
        "    return X_paper_vect"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:18:19.381201Z",
          "iopub.execute_input": "2021-12-07T14:18:19.381658Z",
          "iopub.status.idle": "2021-12-07T14:18:19.405012Z",
          "shell.execute_reply.started": "2021-12-07T14:18:19.381622Z",
          "shell.execute_reply": "2021-12-07T14:18:19.404199Z"
        },
        "trusted": true,
        "id": "-GCaauJErrUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings(embeddings):\n",
        "    np.save(FILE_EMBEDDINGS + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\", embeddings)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:18:19.406319Z",
          "iopub.execute_input": "2021-12-07T14:18:19.406734Z",
          "iopub.status.idle": "2021-12-07T14:18:19.417034Z",
          "shell.execute_reply.started": "2021-12-07T14:18:19.406694Z",
          "shell.execute_reply": "2021-12-07T14:18:19.416228Z"
        },
        "trusted": true,
        "id": "uKe7k6UMrrUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(name):\n",
        "    return np.load(name + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:29:57.709569Z",
          "iopub.execute_input": "2021-12-08T14:29:57.710039Z",
          "iopub.status.idle": "2021-12-08T14:29:57.715144Z",
          "shell.execute_reply.started": "2021-12-08T14:29:57.709999Z",
          "shell.execute_reply": "2021-12-08T14:29:57.714096Z"
        },
        "trusted": true,
        "id": "hSURfs1vrrUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"/kaggle/input/saved-data/\" + FILE_EMBEDDINGS + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\"):\n",
        "    X_paper_vect = load_embeddings(\"/kaggle/input/saved-data/\" + FILE_EMBEDDINGS)\n",
        "    print(\"Embeddings loaded from saved dataset\")\n",
        "elif os.path.isfile(FILE_EMBEDDINGS + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\"):\n",
        "    X_paper_vect = load_embeddings(FILE_EMBEDDINGS)\n",
        "    print(\"Embeddings loaded from temporary save\")\n",
        "else:\n",
        "    X_paper_vect = compute_embeddings(documents)\n",
        "    save_embeddings(X_paper_vect)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:29:58.34215Z",
          "iopub.execute_input": "2021-12-08T14:29:58.342518Z",
          "iopub.status.idle": "2021-12-08T14:30:14.592469Z",
          "shell.execute_reply.started": "2021-12-08T14:29:58.342484Z",
          "shell.execute_reply": "2021-12-08T14:30:14.591524Z"
        },
        "trusted": true,
        "id": "IDozIS4YrrUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This performs the operation of switching from features computed per abstracts to feature computed per author\n",
        "\n",
        "i.e. Assign papers to authors and compute the mean"
      ],
      "metadata": {
        "id": "ghMYHm4ftzn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features_abstract(df, start=0):\n",
        "    total_rows = len(df.index)\n",
        "    X_emb = np.zeros((total_rows, NB_FEATURES_ABSTRACTS))\n",
        "\n",
        "    for j,row in tqdm(df.iterrows()):\n",
        "            i = j - start\n",
        "            author = row['author']\n",
        "            list_papers  = authors[author]\n",
        "            list_indexes = [papers[p] for p in list_papers if p in papers]\n",
        "            list_embeddings = [X_paper_vect[ind] for ind in list_indexes]\n",
        "            embeding = METHOD(np.array(list_embeddings), axis=0)\n",
        "            X_emb[i] = embeding\n",
        "\n",
        "    return X_emb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T17:42:06.365209Z",
          "iopub.execute_input": "2021-12-07T17:42:06.365569Z",
          "iopub.status.idle": "2021-12-07T17:42:06.372419Z",
          "shell.execute_reply.started": "2021-12-07T17:42:06.365529Z",
          "shell.execute_reply": "2021-12-07T17:42:06.371514Z"
        },
        "trusted": true,
        "id": "xXJ-ow_drrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_features_abstract(name, dict_data):\n",
        "    np.save(name + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\", dict_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T17:42:06.373925Z",
          "iopub.execute_input": "2021-12-07T17:42:06.374397Z",
          "iopub.status.idle": "2021-12-07T17:42:06.385142Z",
          "shell.execute_reply.started": "2021-12-07T17:42:06.374358Z",
          "shell.execute_reply": "2021-12-07T17:42:06.384334Z"
        },
        "trusted": true,
        "id": "2po7pwpirrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_features_abstract(name):\n",
        "    return np.load(name + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\", allow_pickle=True).item()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:30:41.550927Z",
          "iopub.execute_input": "2021-12-08T14:30:41.551508Z",
          "iopub.status.idle": "2021-12-08T14:30:41.556635Z",
          "shell.execute_reply.started": "2021-12-08T14:30:41.551468Z",
          "shell.execute_reply": "2021-12-08T14:30:41.555871Z"
        },
        "trusted": true,
        "id": "BJXS3XBbrrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"/kaggle/input/saved-data/\" + FILE_FEATURES_ABSTRACT + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\"):\n",
        "    data = load_features_abstract(\"/kaggle/input/saved-data/\" + FILE_FEATURES_ABSTRACT)\n",
        "    X_emb = data[\"X_emb\"]\n",
        "    X_test_emb  = data[\"X_test_emb\"]\n",
        "    print(\"Embeddings loaded from saved dataset\")\n",
        "elif os.path.isfile(FILE_FEATURES_ABSTRACT + \"_\" + str(NB_FEATURES_ABSTRACTS) + \".npy\"):\n",
        "    data = load_features_abstract(FILE_FEATURES_ABSTRACT)\n",
        "    X_emb = data[\"X_emb\"]\n",
        "    X_test_emb  = data[\"X_test_emb\"]\n",
        "    print(\"Embeddings loaded from temporary save\")\n",
        "else:\n",
        "    # X_emb\n",
        "    X_emb = create_features_abstract(df_train, start=0)\n",
        "    \n",
        "    # X_test_emb\n",
        "    X_test_emb = create_features_abstract(df_test, start=0)\n",
        "    \n",
        "    # Save data\n",
        "    dict_data = {\"X_emb\": X_emb, \"X_test_emb\": X_test_emb}\n",
        "    save_features_abstract(FILE_FEATURES_ABSTRACT, dict_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:31:04.056547Z",
          "iopub.execute_input": "2021-12-08T14:31:04.056842Z",
          "iopub.status.idle": "2021-12-08T14:31:05.320708Z",
          "shell.execute_reply.started": "2021-12-08T14:31:04.056794Z",
          "shell.execute_reply": "2021-12-08T14:31:05.319881Z"
        },
        "trusted": true,
        "id": "gbLZObtdrrUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4. Glove"
      ],
      "metadata": {
        "id": "dPZQNFMcsNVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# source : stackoverflow\n",
        "\n",
        "def compute_embeddings_glove(documents):\n",
        "    tfidfconverter = TfidfVectorizer(max_features=NB_FEATURES_ABSTRACTS-200, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "    X_paper_vect = tfidfconverter.fit_transform(documents).toarray()\n",
        "    \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(documents)\n",
        "\n",
        "    train_sequences = tokenizer.texts_to_sequences(documents)\n",
        "    train_data = pad_sequences(train_sequences, maxlen=500)\n",
        "    EMBEDDING_FILE = '../input/glove6b/glove.6B.200d.txt'\n",
        "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "    embedding_matrix = np.zeros((50000, 200))\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index > 50000 - 1:\n",
        "            break\n",
        "        else:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[index] = embedding_vector\n",
        "                \n",
        "    return X_paper_vect, train_data, embedding_matrix\n",
        "def compute_embeddings_glove_idf(train_data, embedding_matrix):\n",
        "    def batch(iterable, n=1):\n",
        "        l = len(iterable)\n",
        "        for ndx in range(0, l, n):\n",
        "            yield iterable[ndx:min(ndx + n, l)]\n",
        "    X_paper_vect = tf.Variable(tf.zeros([len(train_data), 200]))\n",
        "    l = len(documents)\n",
        "    for i, document in tqdm(enumerate(batch(train_data, 128))):\n",
        "        X_paper_vect[i*128:min(l,(i+1)*128)].assign(tf.math.reduce_mean(Embedding(50000, 200, input_length=500, weights=[embedding_matrix], trainable=False)(document), axis  = 1))\n",
        "    \n",
        "    return X_paper_vect.numpy()"
      ],
      "metadata": {
        "id": "Ya9k6zkSvOuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Vectorization of the graph"
      ],
      "metadata": {
        "id": "N-sThrjGrrUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1. Node2Vec"
      ],
      "metadata": {
        "id": "Q8h5Y-1MrrUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was trained on the other notebook.\n",
        "\n",
        "We load it"
      ],
      "metadata": {
        "id": "7xhVd6nQrrUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained Word2Vec model.\n",
        "import gensim\n",
        "model_n2v = gensim.models.Word2Vec.load(\"../input/saved-data/node2vec.model\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:28:57.341927Z",
          "iopub.execute_input": "2021-12-08T14:28:57.342649Z",
          "iopub.status.idle": "2021-12-08T14:29:02.35446Z",
          "shell.execute_reply.started": "2021-12-08T14:28:57.342609Z",
          "shell.execute_reply": "2021-12-08T14:29:02.353524Z"
        },
        "trusted": true,
        "id": "lxIwmQS6rrUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then compute the embeddings for the train set :"
      ],
      "metadata": {
        "id": "GkcBmG6OrrUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "author_ids_train = [df_train.to_numpy()[i][0] for i in range(n)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:29:03.262968Z",
          "iopub.execute_input": "2021-12-08T14:29:03.263638Z",
          "iopub.status.idle": "2021-12-08T14:29:48.85241Z",
          "shell.execute_reply.started": "2021-12-08T14:29:03.263601Z",
          "shell.execute_reply": "2021-12-08T14:29:48.851612Z"
        },
        "trusted": true,
        "id": "Nzf_lh5drrUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_n2v = []\n",
        "for id in tqdm(author_ids_train[1:]):\n",
        "    X_n2v.append(model_n2v.wv[str(int(id))])\n",
        "X_n2v = np.array(X_n2v)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:29:48.854048Z",
          "iopub.execute_input": "2021-12-08T14:29:48.854313Z",
          "iopub.status.idle": "2021-12-08T14:29:49.610679Z",
          "shell.execute_reply.started": "2021-12-08T14:29:48.854277Z",
          "shell.execute_reply": "2021-12-08T14:29:49.609863Z"
        },
        "trusted": true,
        "id": "W6FoXxwQrrUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And compute the embeddings for the test set"
      ],
      "metadata": {
        "id": "tVLkd2-yrrUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "author_ids_test = [df_test.to_numpy()[i][1] for i in range(n_test)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:29:49.612289Z",
          "iopub.execute_input": "2021-12-08T14:29:49.612575Z",
          "iopub.status.idle": "2021-12-08T14:29:56.725837Z",
          "shell.execute_reply.started": "2021-12-08T14:29:49.612536Z",
          "shell.execute_reply": "2021-12-08T14:29:56.725014Z"
        },
        "trusted": true,
        "id": "tE_d0WAmrrUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_n2v = []\n",
        "for id in tqdm(author_ids_test):\n",
        "    X_test_n2v.append(model_n2v.wv[str(int(id))])\n",
        "X_test_n2v = np.array(X_test_n2v)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:29:57.502103Z",
          "iopub.execute_input": "2021-12-08T14:29:57.502809Z",
          "iopub.status.idle": "2021-12-08T14:29:57.708097Z",
          "shell.execute_reply.started": "2021-12-08T14:29:57.502754Z",
          "shell.execute_reply": "2021-12-08T14:29:57.707302Z"
        },
        "trusted": true,
        "id": "1kf0bk2mrrUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this is really fast to compute, we will not save it."
      ],
      "metadata": {
        "id": "dvt9hUprrrUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Quantile and Variance with random walk"
      ],
      "metadata": {
        "id": "wNHH8P13rrUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we invented a method :\n",
        "\n",
        "For each author, we do ```NB_WALK_PER_NODE``` random walks of length ```LEN_WALK```.\n",
        "\n",
        "We store the *h_index* of each visited node in an array. Then here are the features :\n",
        "- ```nb_quantiles``` of quantiles of the array created in the random walk\n",
        "- the number of unique author visited in the random walk\n",
        "- the variance of the h-index in the random walk\n",
        "- the mean of the h-index in the random walk\n",
        "\n",
        "An we also add features from the node and its neighbors :\n",
        "- degree of the node\n",
        "- core number of the node\n",
        "- ```nb_quantiles``` of quantiles of the array of h-index of its neighbors\n",
        "- the mean of the h-index of its neighbors\n",
        "\n",
        "In the end this gave us 33 features."
      ],
      "metadata": {
        "id": "gXWVgX-CrrUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading the graph and computing the core numbers :"
      ],
      "metadata": {
        "id": "BfqaCdvqrrUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the graph    \n",
        "G = nx.read_edgelist('/kaggle/input/inf554-2021/coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
        "n_nodes = G.number_of_nodes()\n",
        "n_edges = G.number_of_edges() \n",
        "print('Number of nodes:', n_nodes)\n",
        "print('Number of edges:', n_edges)\n",
        "\n",
        "\n",
        "# computes structural features for each node\n",
        "core_number = nx.core_number(G)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T00:34:39.098605Z",
          "iopub.execute_input": "2021-12-08T00:34:39.099304Z",
          "iopub.status.idle": "2021-12-08T00:34:54.629861Z",
          "shell.execute_reply.started": "2021-12-08T00:34:39.099261Z",
          "shell.execute_reply": "2021-12-08T00:34:54.628963Z"
        },
        "trusted": true,
        "id": "4YlO7fkwrrUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the object for the random walk"
      ],
      "metadata": {
        "id": "G4Z_XPkVrrUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rw = BiasedRandomWalk(StellarGraph.from_networkx(G))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:19:01.963957Z",
          "iopub.execute_input": "2021-12-07T14:19:01.964232Z",
          "iopub.status.idle": "2021-12-07T14:19:13.264878Z",
          "shell.execute_reply.started": "2021-12-07T14:19:01.964196Z",
          "shell.execute_reply": "2021-12-07T14:19:13.264032Z"
        },
        "trusted": true,
        "id": "e1yhhUzwrrUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we assign to each node in the graph (an author), its h-index"
      ],
      "metadata": {
        "id": "7nH8dK4frrUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,row in df_train.iterrows():\n",
        "    node = row['author']\n",
        "    G.nodes[node][\"label\"] = row['hindex']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:19:13.26622Z",
          "iopub.execute_input": "2021-12-07T14:19:13.266515Z",
          "iopub.status.idle": "2021-12-07T14:19:21.847668Z",
          "shell.execute_reply.started": "2021-12-07T14:19:13.266478Z",
          "shell.execute_reply": "2021-12-07T14:19:21.84683Z"
        },
        "trusted": true,
        "id": "byVc84mdrrUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a Hashmap associating the id of an author to its h-index"
      ],
      "metadata": {
        "id": "OavCQInmrrUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hindex_from_author = {}\n",
        "\n",
        "for i,row in tqdm(df_train.iterrows()):\n",
        "    hindex_from_author[row[\"author\"]] = row[\"hindex\"]\n",
        "\n",
        "author_with_hindex_unknown = 0\n",
        "for node in tqdm(G.nodes()):\n",
        "    if not(node in hindex_from_author):\n",
        "        hindex_from_author[node] = mediane\n",
        "        author_with_hindex_unknown += 1\n",
        "        \n",
        "percentage_author_with_hindex_unknown = np.round(100 * author_with_hindex_unknown / n_nodes, 2)\n",
        "print(\"Il y a\", percentage_author_with_hindex_unknown, \"% d'auteurs sans h_index\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:19:21.849103Z",
          "iopub.execute_input": "2021-12-07T14:19:21.849368Z",
          "iopub.status.idle": "2021-12-07T14:19:30.736494Z",
          "shell.execute_reply.started": "2021-12-07T14:19:21.849333Z",
          "shell.execute_reply": "2021-12-07T14:19:30.735766Z"
        },
        "trusted": true,
        "id": "vizRuboMrrUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what creates the features, computing the random walk\n",
        "\n",
        "It will take several hours"
      ],
      "metadata": {
        "id": "2I_x5eSLrrUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features_graph(df, build_y = True, start=0):\n",
        "    list_quantiles = QUANTILES\n",
        "    total_rows = len(df.index)\n",
        "    nb_quantiles = len(list_quantiles)\n",
        "    use_std = USESTD\n",
        "    use_mean = USEMEAN\n",
        "    nb_features = NB_FEATURES_GRAPH\n",
        "\n",
        "    print(\"=======================\")\n",
        "    print(\"LEN_WALK         =\", LEN_WALK)\n",
        "    print(\"NB_WALK_PER_NODE =\", NB_WALK_PER_NODE)\n",
        "    print(\"=======================\")\n",
        "\n",
        "    print(\"\\n=====================\")\n",
        "    print(\"NB quantiles =\", nb_quantiles)\n",
        "    print(\"Use STD      =\", use_std)\n",
        "    print(\"Use Mean     =\", use_mean)\n",
        "    print(\"NB features  =\", nb_features)\n",
        "    print(\"=====================\")\n",
        "\n",
        "    print(\"\\nN =\", total_rows)\n",
        "\n",
        "\n",
        "    X = np.zeros((total_rows, nb_features))\n",
        "    if build_y: y = np.zeros(total_rows)\n",
        "    \n",
        "    for j,row in tqdm(df.iterrows()):\n",
        "        i = j - start\n",
        "        node = row['author']\n",
        "    \n",
        "        # ====================== RANDOM WALKS ====================== #\n",
        "        walks = rw.run(\n",
        "              nodes=[node], # root nodes\n",
        "              length=LEN_WALK,  # maximum length of a random walk\n",
        "              n=NB_WALK_PER_NODE,        # number of random walks per root node \n",
        "              p=0.5,       # Defines (unormalised) probability, 1/p, of returning to source node\n",
        "              q=2.0        # Defines (unormalised) probability, 1/q, for moving away from source node\n",
        "        )\n",
        "\n",
        "        # Build list of h-index visited\n",
        "        visited = set()\n",
        "        list_h_index = []\n",
        "        count_visited = 0\n",
        "        for walk in walks:\n",
        "            for node_visited in walk:\n",
        "                if node != node_visited:\n",
        "                    hindex_visited = hindex_from_author[node_visited]\n",
        "                    list_h_index.append(hindex_visited)\n",
        "                    if not(node_visited in visited):\n",
        "                        visited.add(node_visited)\n",
        "                        count_visited += 1\n",
        "        list_h_index = np.array(list_h_index)\n",
        "\n",
        "        # Metrics used\n",
        "        quantiles = np.quantile(list_h_index, list_quantiles)\n",
        "        std = np.std(list_h_index)\n",
        "        mean = np.mean(list_h_index)\n",
        "        count_visited /= LEN_WALK * NB_WALK_PER_NODE\n",
        "        # =========================================================== #\n",
        "\n",
        "\n",
        "        # ====================== NEIGHBORS ====================== #\n",
        "        # Build list of h-index Neighbors\n",
        "        list_h_index = [G.nodes[n][\"label\"]  if \"label\" in G.nodes[n] else mediane for n in G.neighbors(node)]\n",
        "        if len(list_h_index) == 0: list_h_index.append(mediane)\n",
        "        list_h_index = np.array(list_h_index)\n",
        "\n",
        "        # Metrics used\n",
        "        quantiles_neigh = np.quantile(list_h_index, list_quantiles)\n",
        "        std_neigh = np.std(list_h_index)\n",
        "        mean_neigh = np.mean(list_h_index)\n",
        "        # ======================================================= #\n",
        "\n",
        "\n",
        "        # ====================== ADD FEATURES TO XTRAIN ====================== #\n",
        "        X[i,0] = count_visited        # ratio of uniques node visited during random walk\n",
        "        X[i,1] = G.degree(node)       # Nb of neighbors\n",
        "        X[i,2] = core_number[node]    # Core number (don't know what it is)\n",
        "        for j in range(nb_quantiles):\n",
        "            X[i,3+j] = quantiles[j]\n",
        "            X[i,3+j+nb_quantiles] = quantiles_neigh[j]\n",
        "        if use_std:\n",
        "            X[i,3 + 2*nb_quantiles] = std\n",
        "            X[i,4 + 2*nb_quantiles] = std_neigh\n",
        "        if use_mean:\n",
        "            X[i,3 + 2*use_std + 2*nb_quantiles] = mean\n",
        "            X[i,4 + 2*use_std + 2*nb_quantiles] = mean_neigh\n",
        "        # ==================================================================== #\n",
        "\n",
        "\n",
        "        # ====================== ADD YTRAIN ====================== #\n",
        "        if build_y: y[i] = row['hindex']\n",
        "            \n",
        "    if build_y: return X,y\n",
        "    return X"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:19:30.738925Z",
          "iopub.execute_input": "2021-12-07T14:19:30.7392Z",
          "iopub.status.idle": "2021-12-07T14:19:30.756604Z",
          "shell.execute_reply.started": "2021-12-07T14:19:30.739162Z",
          "shell.execute_reply": "2021-12-07T14:19:30.755782Z"
        },
        "trusted": true,
        "id": "olbPMowvrrUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_features_graph(name, dict_data):\n",
        "    np.save(name + \".npy\", dict_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-07T14:19:30.757907Z",
          "iopub.execute_input": "2021-12-07T14:19:30.758252Z",
          "iopub.status.idle": "2021-12-07T14:19:30.770942Z",
          "shell.execute_reply.started": "2021-12-07T14:19:30.758214Z",
          "shell.execute_reply": "2021-12-07T14:19:30.770125Z"
        },
        "trusted": true,
        "id": "_TKIcssyrrUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_features_graph(name):\n",
        "    return np.load(name + \".npy\", allow_pickle=True).item()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:30:14.594212Z",
          "iopub.execute_input": "2021-12-08T14:30:14.594422Z",
          "iopub.status.idle": "2021-12-08T14:30:14.600363Z",
          "shell.execute_reply.started": "2021-12-08T14:30:14.594398Z",
          "shell.execute_reply": "2021-12-08T14:30:14.59948Z"
        },
        "trusted": true,
        "id": "CY3pGOxXrrUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a helper function to load the features if they were saved (from local files of our dataset).\n",
        "\n",
        "Otherwise it performs the heavy computation"
      ],
      "metadata": {
        "id": "22Twg07arrUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(\"/kaggle/input/saved-data/\" + FILE_FEATURES_GRAPH + \".npy\"):\n",
        "    data = load_features_graph(\"/kaggle/input/saved-data/\" + FILE_FEATURES_GRAPH)\n",
        "    X = data[\"X\"]\n",
        "    y = data[\"y\"]\n",
        "    X_test  = data[\"X_test\"]\n",
        "    print(\"Graph features loaded from temporary save\")\n",
        "elif os.path.isfile(FILE_FEATURES_GRAPH + \".npy\"):\n",
        "    data = load_features_graph(FILE_FEATURES_GRAPH)\n",
        "    X = data[\"X\"]\n",
        "    y = data[\"y\"]\n",
        "    X_test  = data[\"X_test\"]\n",
        "    print(\"Graph features loaded from temporary save\")\n",
        "else:\n",
        "    # X, y\n",
        "    X, y = create_features_graph(df_train, build_y = True, start=0)\n",
        "    \n",
        "    # X_test\n",
        "    X_test = create_features_graph(df_test, build_y = False, start=0)\n",
        "    \n",
        "    # Save data\n",
        "    dict_data = {\"X\": X, \"y\": y, \"X_test\": X_test}\n",
        "    save_features_graph(FILE_FEATURES_GRAPH, dict_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:30:14.601812Z",
          "iopub.execute_input": "2021-12-08T14:30:14.602823Z",
          "iopub.status.idle": "2021-12-08T14:30:15.072385Z",
          "shell.execute_reply.started": "2021-12-08T14:30:14.602759Z",
          "shell.execute_reply": "2021-12-08T14:30:15.071515Z"
        },
        "trusted": true,
        "id": "EaTqZld7rrUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Clean Data"
      ],
      "metadata": {
        "id": "QJaQ7dcyucyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Cleaning Nans"
      ],
      "metadata": {
        "id": "XQlBNRXUrrUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean embed nan\n",
        "X_emb_new[np.isnan(X_emb_new)] = 0\n",
        "mean = np.true_divide(X_emb_new.sum(0),(X_emb_new!=0).sum(0))\n",
        "\n",
        "#X[np.isnan(X)] = 0\n",
        "#for i in tqdm(range(len(X))):\n",
        "#    if np.sum(np.abs(X[i])) < 1e-6:"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T13:41:26.187328Z",
          "iopub.execute_input": "2021-12-08T13:41:26.18759Z",
          "iopub.status.idle": "2021-12-08T13:41:26.469574Z",
          "shell.execute_reply.started": "2021-12-08T13:41:26.187558Z",
          "shell.execute_reply": "2021-12-08T13:41:26.468812Z"
        },
        "trusted": true,
        "id": "XlIK7GcerrUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:38:58.837365Z",
          "iopub.execute_input": "2021-12-08T14:38:58.837624Z",
          "iopub.status.idle": "2021-12-08T14:38:58.844348Z",
          "shell.execute_reply.started": "2021-12-08T14:38:58.837595Z",
          "shell.execute_reply": "2021-12-08T14:38:58.843403Z"
        },
        "trusted": true,
        "id": "-8Hy8c1orrUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.true_divide(X_concat.sum(0),(X_concat!=0).sum(0))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:38:56.246056Z",
          "iopub.execute_input": "2021-12-08T14:38:56.246825Z",
          "iopub.status.idle": "2021-12-08T14:38:56.647538Z",
          "shell.execute_reply.started": "2021-12-08T14:38:56.246772Z",
          "shell.execute_reply": "2021-12-08T14:38:56.646739Z"
        },
        "trusted": true,
        "id": "-_4EW43PrrUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_concat[np.isnan(X_concat)] = 0\n",
        "for i in tqdm(range(len(X_concat))):\n",
        "    if np.sum(np.abs(X_concat[i])) < 1e-6:\n",
        "        X_concat[i] = mean\n",
        "#X_test_emb_new[X_test_emb_new==0] = mean\n",
        "X_concat[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:18.50335Z",
          "iopub.execute_input": "2021-12-08T14:39:18.50376Z",
          "iopub.status.idle": "2021-12-08T14:39:20.453497Z",
          "shell.execute_reply.started": "2021-12-08T14:39:18.503725Z",
          "shell.execute_reply": "2021-12-08T14:39:20.452819Z"
        },
        "trusted": true,
        "id": "kDYzrFKVrrUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_concat[np.isnan(X_test_concat)] = 0\n",
        "for i in tqdm(range(len(X_test_concat))):\n",
        "    if np.sum(np.abs(X_test_concat[i])) < 1e-6:\n",
        "        X_test_concat[i] = mean\n",
        "#X_test_emb_new[X_test_emb_new==0] = mean\n",
        "X_test_concat[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:05.091148Z",
          "iopub.execute_input": "2021-12-08T14:39:05.091992Z",
          "iopub.status.idle": "2021-12-08T14:39:05.594832Z",
          "shell.execute_reply.started": "2021-12-08T14:39:05.091944Z",
          "shell.execute_reply": "2021-12-08T14:39:05.594072Z"
        },
        "trusted": true,
        "id": "S1JwovVsrrUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmm = np.mean(X_test_concat, axis=0)\n",
        "print(mmm.shape)\n",
        "print(X_test_concat.shape)\n",
        "X_test_concat = np.concatenate((X_test_concat, mmm.reshape(1,-1)), axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:35:19.147371Z",
          "iopub.execute_input": "2021-12-08T14:35:19.147652Z",
          "iopub.status.idle": "2021-12-08T14:35:19.309653Z",
          "shell.execute_reply.started": "2021-12-08T14:35:19.147624Z",
          "shell.execute_reply": "2021-12-08T14:35:19.308751Z"
        },
        "trusted": true,
        "id": "zjt4vo2lrrUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmm2 = np.mean(X_concat, axis=0)\n",
        "print(mmm2.shape)\n",
        "print(X_concat.shape)\n",
        "X_concat = np.concatenate((X_concat, mmm2.reshape(1,-1)), axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:35:05.44176Z",
          "iopub.execute_input": "2021-12-08T14:35:05.442068Z",
          "iopub.status.idle": "2021-12-08T14:35:06.711444Z",
          "shell.execute_reply.started": "2021-12-08T14:35:05.442038Z",
          "shell.execute_reply": "2021-12-08T14:35:06.710647Z"
        },
        "trusted": true,
        "id": "9dxUzOC6rrUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"X_test_n2v\", X_test_n2v.shape)\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"X_test_emb_new\", X_test_emb_new.shape)\n",
        "print(\"X_test_emb\", X_test_emb.shape)\n",
        "\n",
        "print(\"X_n2v\", X_n2v.shape)\n",
        "print(\"X\", X.shape)\n",
        "print(\"X_emb_new\", X_emb_new.shape)\n",
        "print(\"X_emb\", X_emb.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:37:26.571607Z",
          "iopub.execute_input": "2021-12-08T14:37:26.571889Z",
          "iopub.status.idle": "2021-12-08T14:37:26.582379Z",
          "shell.execute_reply.started": "2021-12-08T14:37:26.571858Z",
          "shell.execute_reply": "2021-12-08T14:37:26.58149Z"
        },
        "trusted": true,
        "id": "z_Sngj8drrUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X node2vec shape:     \", X_n2v.shape)\n",
        "print(\"X graph shape:     \", X.shape)\n",
        "print(\"X embeddings shape:\", X_emb_new.shape)\n",
        "X_concat = np.concatenate((X_n2v, X, X_emb_new, X_emb), axis=1)\n",
        "X_test_concat = np.concatenate((X_test_n2v, X_test, X_test_emb_new, X_test_emb), axis=1)\n",
        "print(\"X concat shape:    \", X_concat.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:37:39.743648Z",
          "iopub.execute_input": "2021-12-08T14:37:39.744237Z",
          "iopub.status.idle": "2021-12-08T14:37:40.513682Z",
          "shell.execute_reply.started": "2021-12-08T14:37:39.744194Z",
          "shell.execute_reply": "2021-12-08T14:37:40.512739Z"
        },
        "trusted": true,
        "id": "bMPnev-5rrUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_concat[:2]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:37:48.024859Z",
          "iopub.execute_input": "2021-12-08T14:37:48.025199Z",
          "iopub.status.idle": "2021-12-08T14:37:48.035857Z",
          "shell.execute_reply.started": "2021-12-08T14:37:48.025165Z",
          "shell.execute_reply": "2021-12-08T14:37:48.034965Z"
        },
        "trusted": true,
        "id": "ZkjMduhtrrUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Checking there is no Nans"
      ],
      "metadata": {
        "id": "xDx-KakGrrUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(np.isnan(X_concat))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:25.451246Z",
          "iopub.execute_input": "2021-12-08T14:39:25.451594Z",
          "iopub.status.idle": "2021-12-08T14:39:25.674324Z",
          "shell.execute_reply.started": "2021-12-08T14:39:25.451553Z",
          "shell.execute_reply": "2021-12-08T14:39:25.673624Z"
        },
        "trusted": true,
        "id": "t88tnL8jrrUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(np.isnan(X_test_concat))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:28.21776Z",
          "iopub.execute_input": "2021-12-08T14:39:28.218358Z",
          "iopub.status.idle": "2021-12-08T14:39:28.266084Z",
          "shell.execute_reply.started": "2021-12-08T14:39:28.21832Z",
          "shell.execute_reply": "2021-12-08T14:39:28.265185Z"
        },
        "trusted": true,
        "id": "vBQCCpBarrUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Splitting"
      ],
      "metadata": {
        "id": "-RTRH74XrrUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.zeros(len(df_train.index))\n",
        "for j,row in tqdm(df_train.iterrows()):\n",
        "    y[j] = row['hindex']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:30.888452Z",
          "iopub.execute_input": "2021-12-08T14:39:30.889198Z",
          "iopub.status.idle": "2021-12-08T14:39:38.185208Z",
          "shell.execute_reply.started": "2021-12-08T14:39:30.889158Z",
          "shell.execute_reply": "2021-12-08T14:39:38.184427Z"
        },
        "trusted": true,
        "id": "liaGozUCrrUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NB_FEATURES = 300 + 33 + 128"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:38.195803Z",
          "iopub.execute_input": "2021-12-08T14:39:38.196371Z",
          "iopub.status.idle": "2021-12-08T14:39:38.204527Z",
          "shell.execute_reply.started": "2021-12-08T14:39:38.196327Z",
          "shell.execute_reply": "2021-12-08T14:39:38.203613Z"
        },
        "trusted": true,
        "id": "fWQX3rfIrrUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_eval, y_train, y_eval = train_test_split(X_concat, y, test_size=0.01, random_state=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:38.209954Z",
          "iopub.execute_input": "2021-12-08T14:39:38.210211Z",
          "iopub.status.idle": "2021-12-08T14:39:38.869464Z",
          "shell.execute_reply.started": "2021-12-08T14:39:38.210177Z",
          "shell.execute_reply": "2021-12-08T14:39:38.868681Z"
        },
        "trusted": true,
        "id": "82ZjF3rVrrUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:38.870796Z",
          "iopub.execute_input": "2021-12-08T14:39:38.871069Z",
          "iopub.status.idle": "2021-12-08T14:39:38.891391Z",
          "shell.execute_reply.started": "2021-12-08T14:39:38.871034Z",
          "shell.execute_reply": "2021-12-08T14:39:38.890697Z"
        },
        "trusted": true,
        "id": "NOl_nFbvrrUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Normalize"
      ],
      "metadata": {
        "id": "uw0Meec5rrUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train = np.mean(X_train, axis=0)\n",
        "std_train = np.std(X_train, axis=0) + 1e-7\n",
        "\n",
        "def transform(X):\n",
        "    return (X - mean_train) / std_train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:38.892582Z",
          "iopub.execute_input": "2021-12-08T14:39:38.892851Z",
          "iopub.status.idle": "2021-12-08T14:39:41.212793Z",
          "shell.execute_reply.started": "2021-12-08T14:39:38.892816Z",
          "shell.execute_reply": "2021-12-08T14:39:41.212005Z"
        },
        "trusted": true,
        "id": "O3Hs5dxQrrUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train_Y = np.median(y_train, axis=0)\n",
        "std_train_Y  = np.std(y_train, axis=0) + 1e-7\n",
        "\n",
        "def transformYLinear(Y):\n",
        "    return (Y - mean_train_Y) / std_train_Y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:41.214184Z",
          "iopub.execute_input": "2021-12-08T14:39:41.214447Z",
          "iopub.status.idle": "2021-12-08T14:39:41.223902Z",
          "shell.execute_reply.started": "2021-12-08T14:39:41.214413Z",
          "shell.execute_reply": "2021-12-08T14:39:41.223183Z"
        },
        "trusted": true,
        "id": "TNCDoHYhrrUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:41.225464Z",
          "iopub.execute_input": "2021-12-08T14:39:41.225729Z",
          "iopub.status.idle": "2021-12-08T14:39:41.244485Z",
          "shell.execute_reply.started": "2021-12-08T14:39:41.225694Z",
          "shell.execute_reply": "2021-12-08T14:39:41.24386Z"
        },
        "trusted": true,
        "id": "9VHt_YeWrrUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverseTransformYLinear(Y):\n",
        "    return Y * std_train_Y + mean_train_Y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:41.245681Z",
          "iopub.execute_input": "2021-12-08T14:39:41.246226Z",
          "iopub.status.idle": "2021-12-08T14:39:41.249863Z",
          "shell.execute_reply.started": "2021-12-08T14:39:41.246192Z",
          "shell.execute_reply": "2021-12-08T14:39:41.249177Z"
        },
        "trusted": true,
        "id": "2o_xcghWrrUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qt = QuantileTransformer(n_quantiles=10, random_state=0)\n",
        "qt.fit_transform(y_train.reshape(-1,1))\n",
        "\n",
        "def transformYQuantile(Y):\n",
        "    return qt.transform(Y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:41.252245Z",
          "iopub.execute_input": "2021-12-08T14:39:41.252672Z",
          "iopub.status.idle": "2021-12-08T14:39:41.286351Z",
          "shell.execute_reply.started": "2021-12-08T14:39:41.252635Z",
          "shell.execute_reply": "2021-12-08T14:39:41.28571Z"
        },
        "trusted": true,
        "id": "JaYfHhizrrUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverseTransformYQuantile(Y):\n",
        "    return qt.inverse_transform(Y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:41.287567Z",
          "iopub.execute_input": "2021-12-08T14:39:41.288006Z",
          "iopub.status.idle": "2021-12-08T14:39:41.291693Z",
          "shell.execute_reply.started": "2021-12-08T14:39:41.287971Z",
          "shell.execute_reply": "2021-12-08T14:39:41.291066Z"
        },
        "trusted": true,
        "id": "i31IVlzmrrUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformY = transformYLinear\n",
        "inverseTransformY = inverseTransformYLinear\n",
        "if not(USE_LINEAR_NORMALIZER_Y):\n",
        "    transformY = transformYQuantile\n",
        "    inverseTransformY = inverseTransformYQuantile"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:41.293046Z",
          "iopub.execute_input": "2021-12-08T14:39:41.293577Z",
          "iopub.status.idle": "2021-12-08T14:39:41.300641Z",
          "shell.execute_reply.started": "2021-12-08T14:39:41.29354Z",
          "shell.execute_reply": "2021-12-08T14:39:41.299923Z"
        },
        "trusted": true,
        "id": "kRSlBwYZrrUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_NORMALIZER:\n",
        "    X_train = transform(X_train)\n",
        "    X_eval  = transform(X_eval)\n",
        "    X_test_concat  = transform(X_test_concat)\n",
        "    y_train = transformY(y_train.reshape(-1,1))\n",
        "    y_eval  = transformY(y_eval.reshape(-1,1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:41.303635Z",
          "iopub.execute_input": "2021-12-08T14:39:41.304261Z",
          "iopub.status.idle": "2021-12-08T14:39:44.20793Z",
          "shell.execute_reply.started": "2021-12-08T14:39:41.304232Z",
          "shell.execute_reply": "2021-12-08T14:39:44.207092Z"
        },
        "trusted": true,
        "id": "ArNWbGxarrUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Network"
      ],
      "metadata": {
        "id": "T01EaXMvrrUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Definition"
      ],
      "metadata": {
        "id": "5zhrtwC7u1pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='tanh', kernel_initializer=\"uniform\", input_shape=(NB_FEATURES + 512,)))\n",
        "model.add(Dense(128, activation='tanh', kernel_initializer=\"uniform\"))\n",
        "model.add(Dense(64, activation='selu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(lr=5*LR, momentum=0.8)#tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:39:58.998797Z",
          "iopub.execute_input": "2021-12-08T14:39:58.999382Z",
          "iopub.status.idle": "2021-12-08T14:39:59.106264Z",
          "shell.execute_reply.started": "2021-12-08T14:39:58.999343Z",
          "shell.execute_reply": "2021-12-08T14:39:59.105479Z"
        },
        "trusted": true,
        "id": "oJLGPs3WrrUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Train"
      ],
      "metadata": {
        "id": "Ib7eTl8lrrUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt.lr.assign(0.5*LR)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:42:03.646247Z",
          "iopub.execute_input": "2021-12-08T14:42:03.646914Z",
          "iopub.status.idle": "2021-12-08T14:42:03.657211Z",
          "shell.execute_reply.started": "2021-12-08T14:42:03.646871Z",
          "shell.execute_reply": "2021-12-08T14:42:03.655847Z"
        },
        "trusted": true,
        "id": "l9GB-2iHrrUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_eval, y_eval), epochs=10, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:42:12.473618Z",
          "iopub.execute_input": "2021-12-08T14:42:12.473897Z"
        },
        "trusted": true,
        "id": "qkILZr3WrrUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Evaluate"
      ],
      "metadata": {
        "id": "i4sOx4MkrrUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_eval_pred = inverseTransformY(model(X_eval))\n",
        "y1 = np.array([np.round(x[0]) for x in list(np.array(y_eval_pred))])\n",
        "y1[y1 <1] = 1\n",
        "y2 = np.array([np.round(x[0]) for x in list(inverseTransformY(y_eval))])\n",
        "acc = np.mean((y1-y2)**2)\n",
        "\n",
        "print(\"Predicted:\", y1[:20])\n",
        "print(\"Ground truth:\", y2[:20])\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:41:27.073712Z",
          "iopub.execute_input": "2021-12-08T14:41:27.074306Z",
          "iopub.status.idle": "2021-12-08T14:41:27.119994Z",
          "shell.execute_reply.started": "2021-12-08T14:41:27.074267Z",
          "shell.execute_reply": "2021-12-08T14:41:27.119235Z"
        },
        "trusted": true,
        "id": "Q7FEcD0urrUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Computing test"
      ],
      "metadata": {
        "id": "mcer4b3DvDUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTING TEST VALUES (for leaderboard)\n",
        "y_test_pred = inverseTransformY(model(X_test_concat))\n",
        "y_pred = np.array([np.round(x[0]) for x in list(np.array(y_test_pred))])\n",
        "y_pred[y_pred <1] = mediane\n",
        "print(y_pred[:20])\n",
        "\n",
        "# write the predictions to file\n",
        "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
        "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Y shape:\", y_pred.shape)\n",
        "print(\"Y pred:\", y_pred[:20])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-08T14:41:48.211391Z",
          "iopub.execute_input": "2021-12-08T14:41:48.212162Z",
          "iopub.status.idle": "2021-12-08T14:41:48.998826Z",
          "shell.execute_reply.started": "2021-12-08T14:41:48.212119Z",
          "shell.execute_reply": "2021-12-08T14:41:48.998063Z"
        },
        "trusted": true,
        "id": "4ff2k4EcrrUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}